{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SageMaker on Local device\n",
    "1. able to request on local device\n",
    "2. able to run on local device\n",
    "    - use `instance_type='local'`\n",
    "3. able to deploy endpoint on local device\n",
    "\n",
    "\n",
    "[usage](https://www.youtube.com/watch?v=K3ngZKF31mc)\n",
    "1. pip install sagemaker\n",
    "2. aws iam list-roles | grep SageMaker-Execution\n",
    "3. set iam role\n",
    "\n",
    "[sample](https://github.com/aws-samples/amazon-sagemaker-local-mode)\n",
    "- set the parameters below for processor or estimator\n",
    "    - set the session `sagemaker_session=sess`\n",
    "    - set the role `role=role`\n",
    "    - set the instance type `instance_type='local'` if want to run on local device\n",
    "- data should be in s3 if it's not local mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.3\n",
      "2.76.0\n",
      "arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001\n",
      "sagemaker-ap-south-1-206034181024\n"
     ]
    }
   ],
   "source": [
    "from util.sagemaker_util import *\n",
    "\n",
    "# change to the iam role on `aws iam list-roles | grep SageMaker-Execution`\n",
    "local_role = \"arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001\"\n",
    "\n",
    "set_environment(\n",
    "    local_role=local_role,\n",
    "    is_local=True,\n",
    "    is_debug=True,\n",
    "    region_name='us-east-1',\n",
    "    profile_name='default',\n",
    "    zip_s3_path=\"s3://bucket/path/to/model.zip\",\n",
    ")\n",
    "upload_pyfiles()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training.\n",
      "Note: if launching for the first time in local mode, container image download might take a few minutes to complete.\n",
      "2022-02-20 14:12:42 Starting - Starting the training job...\n",
      "2022-02-20 14:13:07 Starting - Preparing the instances for trainingProfilerReport-1645366361: InProgress\n",
      "......\n",
      "2022-02-20 14:14:09 Downloading - Downloading input data\n",
      "2022-02-20 14:14:09 Training - Downloading the training image......\n",
      "2022-02-20 14:15:28 Uploading - Uploading generated training model\u001B[34m[2022-02-20 14:15:12.734 ip-10-0-126-143.ap-south-1.compute.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001B[0m\n",
      "\u001B[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001B[0m\n",
      "\u001B[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001B[0m\n",
      "\u001B[34mINFO:sagemaker_xgboost_container.training:Invoking user training script.\u001B[0m\n",
      "\u001B[34mINFO:sagemaker-containers:Module abalone does not provide a setup.py. \u001B[0m\n",
      "\u001B[34mGenerating setup.py\u001B[0m\n",
      "\u001B[34mINFO:sagemaker-containers:Generating setup.cfg\u001B[0m\n",
      "\u001B[34mINFO:sagemaker-containers:Generating MANIFEST.in\u001B[0m\n",
      "\u001B[34mINFO:sagemaker-containers:Installing module with the following command:\u001B[0m\n",
      "\u001B[34m/miniconda3/bin/python3 -m pip install . \u001B[0m\n",
      "\u001B[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mBuilding wheels for collected packages: abalone\n",
      "  Building wheel for abalone (setup.py): started\n",
      "  Building wheel for abalone (setup.py): finished with status 'done'\n",
      "  Created wheel for abalone: filename=abalone-1.0.0-py2.py3-none-any.whl size=6412 sha256=5f56bed8753f17e415b195e7da485a99491ddcc3e224279813ef13961a5fcd15\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-1rbwh5d_/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001B[0m\n",
      "\u001B[34mSuccessfully built abalone\u001B[0m\n",
      "\u001B[34mInstalling collected packages: abalone\u001B[0m\n",
      "\u001B[34mSuccessfully installed abalone-1.0.0\u001B[0m\n",
      "\u001B[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n",
      "\u001B[34mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\u001B[0m\n",
      "\u001B[34mYou should consider upgrading via the '/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\n",
      "\u001B[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001B[0m\n",
      "\u001B[34mINFO:sagemaker-containers:Invoking user script\u001B[0m\n",
      "\u001B[34mTraining Env:\u001B[0m\n",
      "\u001B[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"eta\": \"0.2\",\n",
      "        \"gamma\": \"4\",\n",
      "        \"max_depth\": \"5\",\n",
      "        \"min_child_weight\": \"6\",\n",
      "        \"num_round\": \"50\",\n",
      "        \"objective\": \"reg:squarederror\",\n",
      "        \"subsample\": \"0.7\",\n",
      "        \"verbosity\": \"2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"text/libsvm\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"text/libsvm\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-xgboost-2022-02-20-14-12-38-766\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-south-1-492339234003/sagemaker-xgboost-2022-02-20-14-12-38-766/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"abalone\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"abalone.py\"\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34mEnvironment variables:\u001B[0m\n",
      "\u001B[34mSM_HOSTS=[\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_NETWORK_INTERFACE_NAME=eth0\u001B[0m\n",
      "\u001B[34mSM_HPS={\"eta\":\"0.2\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"}\u001B[0m\n",
      "\u001B[34mSM_USER_ENTRY_POINT=abalone.py\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_PARAMS={}\u001B[0m\n",
      "\u001B[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001B[0m\n",
      "\u001B[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"text/libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001B[0m\n",
      "\u001B[34mSM_CHANNELS=[\"train\",\"validation\"]\u001B[0m\n",
      "\u001B[34mSM_CURRENT_HOST=algo-1\u001B[0m\n",
      "\u001B[34mSM_MODULE_NAME=abalone\u001B[0m\n",
      "\u001B[34mSM_LOG_LEVEL=20\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001B[0m\n",
      "\u001B[34mSM_INPUT_DIR=/opt/ml/input\u001B[0m\n",
      "\u001B[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DIR=/opt/ml/output\u001B[0m\n",
      "\u001B[34mSM_NUM_CPUS=8\u001B[0m\n",
      "\u001B[34mSM_NUM_GPUS=0\u001B[0m\n",
      "\u001B[34mSM_MODEL_DIR=/opt/ml/model\u001B[0m\n",
      "\u001B[34mSM_MODULE_DIR=s3://sagemaker-ap-south-1-492339234003/sagemaker-xgboost-2022-02-20-14-12-38-766/source/sourcedir.tar.gz\u001B[0m\n",
      "\u001B[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"eta\":\"0.2\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"text/libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2022-02-20-14-12-38-766\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-south-1-492339234003/sagemaker-xgboost-2022-02-20-14-12-38-766/source/sourcedir.tar.gz\",\"module_name\":\"abalone\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"abalone.py\"}\u001B[0m\n",
      "\u001B[34mSM_USER_ARGS=[\"--eta\",\"0.2\",\"--gamma\",\"4\",\"--max_depth\",\"5\",\"--min_child_weight\",\"6\",\"--num_round\",\"50\",\"--objective\",\"reg:squarederror\",\"--subsample\",\"0.7\",\"--verbosity\",\"2\"]\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001B[0m\n",
      "\u001B[34mSM_HP_ETA=0.2\u001B[0m\n",
      "\u001B[34mSM_HP_GAMMA=4\u001B[0m\n",
      "\u001B[34mSM_HP_MAX_DEPTH=5\u001B[0m\n",
      "\u001B[34mSM_HP_MIN_CHILD_WEIGHT=6\u001B[0m\n",
      "\u001B[34mSM_HP_NUM_ROUND=50\u001B[0m\n",
      "\u001B[34mSM_HP_OBJECTIVE=reg:squarederror\u001B[0m\n",
      "\u001B[34mSM_HP_SUBSAMPLE=0.7\u001B[0m\n",
      "\u001B[34mSM_HP_VERBOSITY=2\u001B[0m\n",
      "\u001B[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\u001B[0m\n",
      "\u001B[34mInvoking script with the following command:\u001B[0m\n",
      "\u001B[34m/miniconda3/bin/python3 -m abalone --eta 0.2 --gamma 4 --max_depth 5 --min_child_weight 6 --num_round 50 --objective reg:squarederror --subsample 0.7 --verbosity 2\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[0]#011train-rmse:8.09086#011validation-rmse:8.09086\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[1]#011train-rmse:6.61128#011validation-rmse:6.61128\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[2]#011train-rmse:5.44558#011validation-rmse:5.44558\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 6 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[3]#011train-rmse:4.54894#011validation-rmse:4.54894\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 8 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[4]#011train-rmse:3.85380#011validation-rmse:3.85380\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 4 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[5]#011train-rmse:3.32450#011validation-rmse:3.32450\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 6 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[6]#011train-rmse:2.92907#011validation-rmse:2.92907\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[7]#011train-rmse:2.64925#011validation-rmse:2.64925\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[8]#011train-rmse:2.43828#011validation-rmse:2.43828\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[9]#011train-rmse:2.28504#011validation-rmse:2.28504\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[10]#011train-rmse:2.17757#011validation-rmse:2.17757\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[11]#011train-rmse:2.10257#011validation-rmse:2.10257\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[12]#011train-rmse:2.04681#011validation-rmse:2.04681\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[13]#011train-rmse:2.00737#011validation-rmse:2.00737\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[14]#011train-rmse:1.97778#011validation-rmse:1.97778\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[15]#011train-rmse:1.95060#011validation-rmse:1.95060\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[16]#011train-rmse:1.93036#011validation-rmse:1.93036\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[17]#011train-rmse:1.91997#011validation-rmse:1.91997\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[18]#011train-rmse:1.90255#011validation-rmse:1.90255\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[19]#011train-rmse:1.88461#011validation-rmse:1.88461\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[20]#011train-rmse:1.87660#011validation-rmse:1.87660\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[21]#011train-rmse:1.86282#011validation-rmse:1.86282\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[22]#011train-rmse:1.85499#011validation-rmse:1.85499\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[23]#011train-rmse:1.84877#011validation-rmse:1.84877\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 8 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[24]#011train-rmse:1.84014#011validation-rmse:1.84014\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[25]#011train-rmse:1.83703#011validation-rmse:1.83703\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[26]#011train-rmse:1.82825#011validation-rmse:1.82825\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[27]#011train-rmse:1.82615#011validation-rmse:1.82615\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[28]#011train-rmse:1.81786#011validation-rmse:1.81786\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[29]#011train-rmse:1.81118#011validation-rmse:1.81118\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 6 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[30]#011train-rmse:1.80298#011validation-rmse:1.80298\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[31]#011train-rmse:1.79703#011validation-rmse:1.79703\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[32]#011train-rmse:1.78973#011validation-rmse:1.78973\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 8 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[33]#011train-rmse:1.78096#011validation-rmse:1.78096\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 12 extra nodes, 4 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[34]#011train-rmse:1.77939#011validation-rmse:1.77939\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 16 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[35]#011train-rmse:1.77712#011validation-rmse:1.77712\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 28 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[36]#011train-rmse:1.77266#011validation-rmse:1.77266\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 4 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[37]#011train-rmse:1.76878#011validation-rmse:1.76878\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 6 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[38]#011train-rmse:1.76343#011validation-rmse:1.76343\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 4 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[39]#011train-rmse:1.75774#011validation-rmse:1.75774\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 4 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[40]#011train-rmse:1.75110#011validation-rmse:1.75110\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 6 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[41]#011train-rmse:1.74668#011validation-rmse:1.74668\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[42]#011train-rmse:1.74404#011validation-rmse:1.74404\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 6 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[43]#011train-rmse:1.74232#011validation-rmse:1.74232\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[44]#011train-rmse:1.73694#011validation-rmse:1.73694\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[45]#011train-rmse:1.73464#011validation-rmse:1.73464\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 2 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[46]#011train-rmse:1.72677#011validation-rmse:1.72677\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[47]#011train-rmse:1.72361#011validation-rmse:1.72361\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[48]#011train-rmse:1.71716#011validation-rmse:1.71716\u001B[0m\n",
      "\u001B[34m[14:15:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001B[0m\n",
      "\u001B[34m[49]#011train-rmse:1.70623#011validation-rmse:1.70623\u001B[0m\n",
      "\n",
      "2022-02-20 14:15:48 Completed - Training job completed\n",
      "Training seconds: 97\n",
      "Billable seconds: 97\n",
      "Completed model training\n",
      "s3://sagemaker-ap-south-1-492339234003/sagemaker-xgboost-2022-02-20-14-12-38-766/output/model.tar.gz\n",
      "Deploying endpoint in local mode\n",
      "------------!Prediction: [['6.8532515', '0.0', '-0.3545924', '-0.12613766', '-0.36350462', '-0.37387854', '-0.83996755', '1.5954899', '0.38270319', '-2.9971762', '9.930313']]\n",
      "Prediction: [['14.508704', '0.0', '-0.0045526065', '-0.07738679', '0.023501989', '0.35198748', '0.9640153', '0.92003435', '0.040878277', '2.3599126', '9.930313']]\n",
      "About to delete the endpoint to stop paying (if in cloud mode).\n"
     ]
    }
   ],
   "source": [
    "# This is a sample Python program that trains a simple XGBoost model on Abalone dataset.\n",
    "# This implementation will work on your *local computer* or in the *AWS Cloud*.\n",
    "#\n",
    "# Prerequisites:\n",
    "#   1. Install required Python packages:\n",
    "#      `pip install -r requirements.txt`\n",
    "#   2. Docker Desktop installed and running on your computer:\n",
    "#      `docker ps`\n",
    "#   3. You should have AWS credentials configured on your local machine\n",
    "#      in order to be able to pull the docker image from ECR.\n",
    "###############################################################################################\n",
    "\n",
    "from sagemaker import TrainingInput\n",
    "from sagemaker.xgboost import XGBoost, XGBoostModel\n",
    "\n",
    "def do_inference_on_local_endpoint(predictor, libsvm_str):\n",
    "    label, *features = libsvm_str.strip().split()\n",
    "    predictions = predictor.predict(\" \".join([\"-99\"] + features))  # use dummy label -99\n",
    "    print(\"Prediction: {}\".format(predictions))\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('Starting model training.')\n",
    "    print('Note: if launching for the first time in local mode, container image download might take a few minutes to complete.')\n",
    "\n",
    "    hyperparameters = {\n",
    "        \"max_depth\": \"5\",\n",
    "        \"eta\": \"0.2\",\n",
    "        \"gamma\": \"4\",\n",
    "        \"min_child_weight\": \"6\",\n",
    "        \"subsample\": \"0.7\",\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"num_round\": \"50\",\n",
    "        \"verbosity\": \"2\",\n",
    "    }\n",
    "\n",
    "    xgb_script_mode_estimator = XGBoost(\n",
    "        entry_point=\"./code/abalone.py\",\n",
    "        hyperparameters=hyperparameters,\n",
    "        instance_count=1,\n",
    "        framework_version=\"1.2-1\",\n",
    "        **param('ml.m5.2xlarge')\n",
    "    )\n",
    "\n",
    "    train_input = TrainingInput(\"s3://some_bucket/abalone\", content_type=\"text/libsvm\")\n",
    "\n",
    "    xgb_script_mode_estimator.fit({\"train\": train_input, \"validation\": train_input})\n",
    "\n",
    "    print('Completed model training')\n",
    "\n",
    "    model_data = xgb_script_mode_estimator.model_data\n",
    "    print(model_data)\n",
    "\n",
    "    xgb_inference_model = XGBoostModel(\n",
    "        model_data=model_data,\n",
    "        entry_point=\"./code/inference.py\",\n",
    "        framework_version=\"1.2-1\",\n",
    "        **param()\n",
    "    )\n",
    "\n",
    "    print('Deploying endpoint in local mode')\n",
    "    predictor = xgb_inference_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=instance_type('ml.m5.2xlarge'),\n",
    "    )\n",
    "\n",
    "    a_young_abalone = \"6 1:3 2:0.37 3:0.29 4:0.095 5:0.249 6:0.1045 7:0.058 8:0.067\"\n",
    "    do_inference_on_local_endpoint(predictor, a_young_abalone)\n",
    "\n",
    "    an_old_abalone = \"15 1:1 2:0.655 3:0.53 4:0.175 5:1.2635 6:0.486 7:0.2635 8:0.415\"\n",
    "    do_inference_on_local_endpoint(predictor, an_old_abalone)\n",
    "\n",
    "    if not is_local():\n",
    "        print('About to delete the endpoint to stop paying (if in cloud mode).')\n",
    "        predictor.delete_endpoint(predictor.endpoint_name)\n",
    "\n",
    "\n",
    "main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pysparkenv",
   "language": "python",
   "display_name": "Python (pysparkenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}